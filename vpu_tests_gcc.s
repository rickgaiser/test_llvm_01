	.file	1 "vpu_tests.c"
	.section .mdebug.abiN32
	.previous
	.nan	legacy
	.module	singlefloat
	.module	oddspreg
	.module	arch=r5900
	.abicalls
	.option	pic0
	.text
	.align	2
	.align	3
	.globl	vpu_vector_add
	.set	nomips16
	.set	nomicromips
	.ent	vpu_vector_add
	.type	vpu_vector_add, @function
vpu_vector_add:
	.frame	$sp,0,$31		# vars= 0, regs= 0/0, args= 0, gp= 0
	.mask	0x00000000,0
	.fmask	0x00000000,0
	blez	$7,.L6
	sll	$7,$7,4
	addu	$7,$4,$7
	.align	3
.L3:
	lwc1	$f3,4($4)
	addiu	$5,$5,16
	lwc1	$f7,-12($5)
	addiu	$6,$6,16
	lwc1	$f6,-8($5)
	addiu	$4,$4,16
	lwc1	$f2,-8($4)
	lwc1	$f5,-4($5)
	lwc1	$f0,-16($5)
	lwc1	$f1,-4($4)
	lwc1	$f4,-16($4)
	add.s	$f3,$f3,$f7
	add.s	$f2,$f2,$f6
	add.s	$f1,$f1,$f5
	add.s	$f0,$f0,$f4
	swc1	$f3,-12($6)
	swc1	$f2,-8($6)
	swc1	$f1,-4($6)
	swc1	$f0,-16($6)
	bne	$7,$4,.L3
.L6:
	jr	$31
	.end	vpu_vector_add
	.size	vpu_vector_add, .-vpu_vector_add
	.align	2
	.align	3
	.globl	vpu_vector_mul
	.set	nomips16
	.set	nomicromips
	.ent	vpu_vector_mul
	.type	vpu_vector_mul, @function
vpu_vector_mul:
	.frame	$sp,0,$31		# vars= 0, regs= 0/0, args= 0, gp= 0
	.mask	0x00000000,0
	.fmask	0x00000000,0
	blez	$7,.L11
	sll	$7,$7,4
	addu	$7,$4,$7
	.align	3
.L9:
	lwc1	$f3,4($4)
	addiu	$5,$5,16
	lwc1	$f7,-12($5)
	addiu	$6,$6,16
	lwc1	$f6,-8($5)
	addiu	$4,$4,16
	lwc1	$f2,-8($4)
	lwc1	$f5,-4($5)
	lwc1	$f0,-16($5)
	lwc1	$f1,-4($4)
	lwc1	$f4,-16($4)
	mul.s	$f3,$f3,$f7
	mul.s	$f2,$f2,$f6
	mul.s	$f1,$f1,$f5
	mul.s	$f0,$f0,$f4
	swc1	$f3,-12($6)
	swc1	$f2,-8($6)
	swc1	$f1,-4($6)
	swc1	$f0,-16($6)
	bne	$7,$4,.L9
.L11:
	jr	$31
	.end	vpu_vector_mul
	.size	vpu_vector_mul, .-vpu_vector_mul
	.align	2
	.align	3
	.globl	vpu_vector_madd
	.set	nomips16
	.set	nomicromips
	.ent	vpu_vector_madd
	.type	vpu_vector_madd, @function
vpu_vector_madd:
	.frame	$sp,0,$31		# vars= 0, regs= 0/0, args= 0, gp= 0
	.mask	0x00000000,0
	.fmask	0x00000000,0
	blez	$8,.L16
	sll	$8,$8,4
	move	$2,$0
	.align	3
.L14:
	addu	$10,$4,$2
	addu	$11,$5,$2
	lwc1	$f7,4($11)
	addu	$9,$6,$2
	lwc1	$f6,8($10)
	addu	$3,$7,$2
	lwc1	$f5,12($10)
	addiu	$2,$2,16
	lwc1	$f4,0($10)
	lwc1	$f3,4($10)
	lwc1	$f2,8($11)
	lwc1	$f1,12($11)
	lwc1	$f0,0($11)
	mul.s	$f3,$f3,$f7
	mul.s	$f2,$f2,$f6
	mul.s	$f1,$f1,$f5
	mul.s	$f0,$f0,$f4
	lwc1	$f7,4($9)
	lwc1	$f6,8($9)
	lwc1	$f5,12($9)
	lwc1	$f4,0($9)
	add.s	$f3,$f3,$f7
	add.s	$f2,$f2,$f6
	add.s	$f1,$f1,$f5
	add.s	$f0,$f0,$f4
	swc1	$f3,4($3)
	swc1	$f2,8($3)
	swc1	$f1,12($3)
	swc1	$f0,0($3)
	bne	$8,$2,.L14
.L16:
	jr	$31
	.end	vpu_vector_madd
	.size	vpu_vector_madd, .-vpu_vector_madd
	.align	2
	.align	3
	.globl	vpu_matrix_vector_multiply
	.set	nomips16
	.set	nomicromips
	.ent	vpu_matrix_vector_multiply
	.type	vpu_matrix_vector_multiply, @function
vpu_matrix_vector_multiply:
	.frame	$sp,112,$31		# vars= 80, regs= 0/6, args= 0, gp= 0
	.mask	0x00000000,0
	.fmask	0x55500000,-4
	blez	$7,.L39
	addiu	$sp,$sp,-112
	andi	$2,$7,0x1
	lwc1	$f21,12($4)
	swc1	$f30,108($sp)
	swc1	$f28,104($sp)
	swc1	$f26,100($sp)
	swc1	$f24,96($sp)
	swc1	$f22,92($sp)
	swc1	$f20,88($sp)
	lwc1	$f5,8($4)
	lwc1	$f6,4($4)
	lwc1	$f7,0($4)
	lwc1	$f19,28($4)
	lwc1	$f14,24($4)
	lwc1	$f15,20($4)
	lwc1	$f16,16($4)
	lwc1	$f18,44($4)
	lwc1	$f11,40($4)
	lwc1	$f12,36($4)
	lwc1	$f13,32($4)
	lwc1	$f17,60($4)
	lwc1	$f8,56($4)
	lwc1	$f9,52($4)
	lwc1	$f10,48($4)
	beq	$2,$0,.L20
	lwc1	$f4,0($5)
	addiu	$7,$7,-1
	lwc1	$f0,4($5)
	mul.s	$f24,$f4,$f6
	mul.s	$f23,$f4,$f5
	mul.s	$f22,$f4,$f21
	mul.s	$f20,$f4,$f7
	mul.s	$f3,$f0,$f15
	mul.s	$f2,$f0,$f14
	mul.s	$f1,$f0,$f19
	mul.s	$f0,$f0,$f16
	lwc1	$f4,8($5)
	add.s	$f3,$f3,$f24
	add.s	$f2,$f2,$f23
	mul.s	$f24,$f4,$f12
	mul.s	$f23,$f4,$f11
	add.s	$f1,$f1,$f22
	add.s	$f0,$f0,$f20
	mul.s	$f22,$f4,$f18
	mul.s	$f20,$f4,$f13
	lwc1	$f4,12($5)
	add.s	$f3,$f3,$f24
	add.s	$f2,$f2,$f23
	mul.s	$f24,$f4,$f9
	mul.s	$f23,$f4,$f8
	add.s	$f1,$f1,$f22
	add.s	$f0,$f0,$f20
	mul.s	$f22,$f4,$f17
	mul.s	$f4,$f4,$f10
	add.s	$f3,$f3,$f24
	add.s	$f2,$f2,$f23
	add.s	$f1,$f1,$f22
	add.s	$f0,$f0,$f4
	swc1	$f3,4($6)
	swc1	$f2,8($6)
	swc1	$f1,12($6)
	swc1	$f0,0($6)
	beq	$7,$0,.L17
	addiu	$5,$5,16
	addiu	$6,$6,16
.L20:
	lwc1	$f2,0($5)
	li	$2,2			# 0x2
	lwc1	$f0,20($5)
	mul.s	$f3,$f2,$f7
	lwc1	$f1,4($5)
	mul.s	$f31,$f2,$f6
	mul.s	$f30,$f2,$f5
	mul.s	$f27,$f2,$f21
	mfc1	$3,$f3
	mul.s	$f4,$f0,$f16
	mul.s	$f3,$f0,$f15
	mul.s	$f2,$f0,$f14
	mul.s	$f0,$f0,$f19
	lwc1	$f24,16($5)
	mul.s	$f23,$f1,$f16
	lwc1	$f25,8($5)
	mul.s	$f26,$f24,$f5
	mfc1	$8,$f0
	mtc1	$3,$f0
	mul.s	$f29,$f24,$f7
	add.s	$f23,$f23,$f0
	mul.s	$f0,$f25,$f13
	mfc1	$4,$f26
	mul.s	$f28,$f24,$f6
	mul.s	$f26,$f24,$f21
	mfc1	$3,$f0
	lwc1	$f24,24($5)
	mtc1	$4,$f0
	mul.s	$f22,$f1,$f15
	add.s	$f2,$f2,$f0
	mul.s	$f0,$f24,$f11
	mul.s	$f20,$f1,$f14
	mul.s	$f1,$f1,$f19
	add.s	$f22,$f22,$f31
	mfc1	$4,$f0
	mtc1	$8,$f0
	mul.s	$f31,$f25,$f12
	add.s	$f0,$f0,$f26
	add.s	$f20,$f20,$f30
	add.s	$f1,$f1,$f27
	mul.s	$f30,$f25,$f11
	mul.s	$f27,$f25,$f18
	mfc1	$8,$f0
	lwc1	$f25,12($5)
	mtc1	$3,$f0
	mul.s	$f26,$f24,$f18
	add.s	$f23,$f23,$f0
	mul.s	$f0,$f25,$f10
	add.s	$f4,$f4,$f29
	add.s	$f3,$f3,$f28
	mul.s	$f29,$f24,$f13
	mul.s	$f28,$f24,$f12
	mfc1	$3,$f0
	mtc1	$4,$f0
	lwc1	$f24,28($5)
	add.s	$f22,$f22,$f31
	add.s	$f20,$f20,$f30
	mul.s	$f31,$f25,$f9
	mul.s	$f30,$f25,$f8
	add.s	$f1,$f1,$f27
	add.s	$f2,$f2,$f0
	mul.s	$f25,$f25,$f17
	mtc1	$8,$f0
	add.s	$f4,$f4,$f29
	add.s	$f3,$f3,$f28
	mul.s	$f29,$f24,$f10
	mul.s	$f28,$f24,$f9
	mul.s	$f27,$f24,$f8
	add.s	$f0,$f0,$f26
	mul.s	$f24,$f24,$f17
	mtc1	$3,$f26
	add.s	$f22,$f22,$f31
	add.s	$f23,$f23,$f26
	add.s	$f20,$f20,$f30
	add.s	$f1,$f1,$f25
	add.s	$f4,$f4,$f29
	add.s	$f3,$f3,$f28
	add.s	$f2,$f2,$f27
	add.s	$f0,$f0,$f24
	swc1	$f23,64($sp)
	swc1	$f22,68($sp)
	swc1	$f20,72($sp)
	swc1	$f1,76($sp)
	ld	$9,64($sp)
	swc1	$f4,64($sp)
	ld	$10,72($sp)
	swc1	$f3,68($sp)
	swc1	$f2,72($sp)
	swc1	$f0,76($sp)
	ld	$11,64($sp)
	ld	$8,72($sp)
	beq	$7,$2,.L38
	ld	$12,40($5)
	addiu	$3,$7,-4
	ld	$13,32($5)
	ld	$4,48($5)
	ld	$2,56($5)
	sd	$12,56($sp)
	move	$12,$8
	sd	$13,48($sp)
	sd	$4,32($sp)
	sd	$2,40($sp)
	blez	$3,.L23
	addiu	$4,$7,-5
	li	$2,-2			# 0xfffffffffffffffe
	and	$2,$4,$2
	addiu	$7,$7,-6
	subu	$7,$7,$2
	addiu	$5,$5,64
	move	$2,$6
	.align	3
.L24:
	lwc1	$f1,48($sp)
	sd	$12,24($2)
	lwc1	$f0,36($sp)
	sd	$10,8($2)
	mul.s	$f3,$f1,$f7
	ld	$10,16($5)
	lwc1	$f2,52($sp)
	sd	$11,16($2)
	mul.s	$f23,$f1,$f6
	ld	$11,0($5)
	mul.s	$f31,$f1,$f5
	sd	$11,48($sp)
	mul.s	$f20,$f1,$f21
	sd	$9,0($2)
	mul.s	$f1,$f0,$f15
	ld	$9,8($5)
	mul.s	$f22,$f0,$f14
	ld	$8,24($5)
	mfc1	$12,$f3
	addiu	$3,$3,-2
	mul.s	$f3,$f0,$f16
	addiu	$2,$2,32
	mul.s	$f0,$f0,$f19
	addiu	$5,$5,32
	mul.s	$f27,$f2,$f16
	lwc1	$f29,56($sp)
	lwc1	$f4,32($sp)
	sd	$10,32($sp)
	mfc1	$13,$f0
	mtc1	$12,$f0
	mul.s	$f30,$f4,$f7
	add.s	$f27,$f27,$f0
	mul.s	$f0,$f29,$f13
	mfc1	$10,$f1
	mul.s	$f26,$f2,$f15
	mul.s	$f25,$f2,$f14
	mfc1	$12,$f0
	mul.s	$f0,$f29,$f12
	mul.s	$f24,$f2,$f19
	mul.s	$f1,$f4,$f5
	mul.s	$f2,$f4,$f6
	lwc1	$f28,40($sp)
	mfc1	$11,$f0
	mtc1	$10,$f0
	add.s	$f26,$f26,$f23
	add.s	$f25,$f25,$f31
	mul.s	$f23,$f29,$f18
	mul.s	$f31,$f29,$f11
	add.s	$f24,$f24,$f20
	mul.s	$f29,$f28,$f12
	lwc1	$f20,60($sp)
	sd	$9,56($sp)
	add.s	$f3,$f3,$f30
	add.s	$f2,$f2,$f0
	mul.s	$f30,$f28,$f13
	mul.s	$f0,$f28,$f11
	add.s	$f1,$f1,$f22
	mul.s	$f22,$f28,$f18
	mtc1	$12,$f28
	mul.s	$f4,$f4,$f21
	add.s	$f27,$f27,$f28
	mul.s	$f28,$f20,$f10
	mfc1	$10,$f0
	mtc1	$13,$f0
	add.s	$f25,$f25,$f31
	mfc1	$9,$f28
	mtc1	$11,$f28
	add.s	$f0,$f0,$f4
	add.s	$f26,$f26,$f28
	mul.s	$f28,$f20,$f9
	lwc1	$f4,44($sp)
	sd	$8,40($sp)
	mul.s	$f31,$f20,$f8
	add.s	$f24,$f24,$f23
	add.s	$f0,$f0,$f22
	mfc1	$8,$f28
	mtc1	$9,$f22
	mtc1	$10,$f23
	mul.s	$f20,$f20,$f17
	add.s	$f3,$f3,$f30
	add.s	$f2,$f2,$f29
	mul.s	$f30,$f4,$f10
	mul.s	$f29,$f4,$f9
	mul.s	$f28,$f4,$f8
	add.s	$f27,$f27,$f22
	add.s	$f1,$f1,$f23
	mul.s	$f4,$f4,$f17
	mtc1	$8,$f22
	add.s	$f25,$f25,$f31
	add.s	$f26,$f26,$f22
	add.s	$f24,$f24,$f20
	add.s	$f3,$f3,$f30
	add.s	$f2,$f2,$f29
	add.s	$f1,$f1,$f28
	add.s	$f0,$f0,$f4
	swc1	$f27,64($sp)
	swc1	$f26,68($sp)
	swc1	$f25,72($sp)
	swc1	$f24,76($sp)
	ld	$9,64($sp)
	swc1	$f3,64($sp)
	ld	$10,72($sp)
	swc1	$f2,68($sp)
	swc1	$f1,72($sp)
	swc1	$f0,76($sp)
	ld	$11,64($sp)
	ld	$12,72($sp)
	bne	$3,$7,.L24
	srl	$4,$4,1
	addiu	$6,$6,32
	sll	$4,$4,5
	addu	$6,$4,$6
.L23:
	lwc1	$f20,36($sp)
	sd	$9,0($6)
	lwc1	$f1,48($sp)
	sd	$10,8($6)
	lwc1	$f22,52($sp)
	sd	$11,16($6)
	lwc1	$f0,32($sp)
	sd	$12,24($6)
	mul.s	$f25,$f1,$f7
	mul.s	$f24,$f1,$f6
	mul.s	$f23,$f22,$f14
	mul.s	$f4,$f22,$f16
	mul.s	$f3,$f22,$f15
	mul.s	$f2,$f1,$f5
	mul.s	$f16,$f20,$f16
	mul.s	$f15,$f20,$f15
	mul.s	$f14,$f20,$f14
	mul.s	$f1,$f1,$f21
	mul.s	$f22,$f22,$f19
	mul.s	$f7,$f0,$f7
	mul.s	$f6,$f0,$f6
	mul.s	$f5,$f0,$f5
	mul.s	$f0,$f0,$f21
	mul.s	$f21,$f20,$f19
	lwc1	$f20,56($sp)
	lwc1	$f19,40($sp)
	add.s	$f4,$f4,$f25
	add.s	$f3,$f3,$f24
	mul.s	$f25,$f20,$f13
	mul.s	$f24,$f20,$f12
	add.s	$f2,$f2,$f23
	add.s	$f7,$f7,$f16
	mul.s	$f23,$f20,$f11
	mul.s	$f13,$f19,$f13
	mul.s	$f20,$f20,$f18
	add.s	$f6,$f6,$f15
	mul.s	$f12,$f19,$f12
	add.s	$f5,$f5,$f14
	mul.s	$f11,$f19,$f11
	add.s	$f0,$f0,$f21
	mul.s	$f19,$f19,$f18
	add.s	$f1,$f1,$f22
	lwc1	$f15,60($sp)
	lwc1	$f14,44($sp)
	mul.s	$f21,$f15,$f10
	mul.s	$f18,$f15,$f9
	mul.s	$f16,$f15,$f8
	mul.s	$f10,$f14,$f10
	mul.s	$f9,$f14,$f9
	mul.s	$f8,$f14,$f8
	add.s	$f4,$f4,$f25
	add.s	$f3,$f3,$f24
	add.s	$f2,$f2,$f23
	add.s	$f1,$f1,$f20
	mul.s	$f15,$f15,$f17
	add.s	$f7,$f7,$f13
	add.s	$f6,$f6,$f12
	add.s	$f5,$f5,$f11
	add.s	$f0,$f0,$f19
	mul.s	$f14,$f14,$f17
	add.s	$f4,$f4,$f21
	add.s	$f3,$f3,$f18
	add.s	$f2,$f2,$f16
	add.s	$f1,$f1,$f15
	add.s	$f7,$f7,$f10
	add.s	$f6,$f6,$f9
	add.s	$f5,$f5,$f8
	add.s	$f0,$f0,$f14
	swc1	$f4,32($6)
	swc1	$f3,36($6)
	swc1	$f2,40($6)
	swc1	$f1,44($6)
	swc1	$f7,48($6)
	swc1	$f6,52($6)
	swc1	$f5,56($6)
	swc1	$f0,60($6)
.L17:
	lwc1	$f30,108($sp)
	lwc1	$f28,104($sp)
	lwc1	$f26,100($sp)
	lwc1	$f24,96($sp)
	lwc1	$f22,92($sp)
	lwc1	$f20,88($sp)
	.set	noreorder
	.set	nomacro
	jr	$31
	addiu	$sp,$sp,112
	.set	macro
	.set	reorder

.L39:
	jr	$31
	.align	3
.L38:
	lwc1	$f30,108($sp)
	sd	$9,0($6)
	lwc1	$f28,104($sp)
	sd	$10,8($6)
	lwc1	$f26,100($sp)
	sd	$11,16($6)
	lwc1	$f24,96($sp)
	sd	$8,24($6)
	lwc1	$f22,92($sp)
	lwc1	$f20,88($sp)
	.set	noreorder
	.set	nomacro
	jr	$31
	addiu	$sp,$sp,112
	.set	macro
	.set	reorder

	.end	vpu_matrix_vector_multiply
	.size	vpu_matrix_vector_multiply, .-vpu_matrix_vector_multiply
	.align	2
	.align	3
	.globl	vpu_dot_product
	.set	nomips16
	.set	nomicromips
	.ent	vpu_dot_product
	.type	vpu_dot_product, @function
vpu_dot_product:
	.frame	$sp,0,$31		# vars= 0, regs= 0/0, args= 0, gp= 0
	.mask	0x00000000,0
	.fmask	0x00000000,0
	blez	$7,.L44
	sll	$7,$7,4
	addu	$7,$4,$7
	.align	3
.L42:
	lwc1	$f0,0($5)
	addiu	$5,$5,16
	lwc1	$f3,-12($5)
	addiu	$4,$4,16
	lwc1	$f2,-16($4)
	addiu	$6,$6,4
	lwc1	$f1,-12($4)
	mul.s	$f0,$f0,$f2
	mul.s	$f3,$f1,$f3
	lwc1	$f2,-8($4)
	lwc1	$f4,-8($5)
	lwc1	$f1,-4($5)
	add.s	$f0,$f0,$f3
	mul.s	$f2,$f2,$f4
	lwc1	$f3,-4($4)
	mul.s	$f1,$f1,$f3
	add.s	$f0,$f0,$f2
	add.s	$f0,$f0,$f1
	swc1	$f0,-4($6)
	bne	$7,$4,.L42
.L44:
	jr	$31
	.end	vpu_dot_product
	.size	vpu_dot_product, .-vpu_dot_product
	.ident	"GCC: (GNU) 15.2.0"
